#%matplotlib notebook
#%matplotlib inline
#matplotlib.interactive(True)
%gui qt

import matplotlib
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
import matplotlib.image as mpimg
from matplotlib import cm
from matplotlib import rcParams
import matplotlib.animation as animation
from matplotlib.widgets import Slider
import sys
sys.path.append("/lib/python2.7/site-packages/")
sys.path.append("/lib64/python2.7/site-packages/")
sys.path.append("/anaconda2/lib/python2.7/site-packages")
sys.path.append("/home/simon/Documents/Projects/Astrophysics/LOFAR/Tomography/tools21cm-master/src/")
import time
import scipy
from IPython.display import HTML
import pandas as pd
import numpy as np
import math
import cmath
from mpl_toolkits.mplot3d import axes3d, Axes3D #<-- Note the capitalization! 
import json
import ipyvolume as ipv
from astropy.utils.data import get_pkg_data_filename
from astropy.io import fits  # We use fits to open the actual data file
from random import uniform, shuffle, randint
from pyquaternion import Quaternion
from scipy import stats
import csv
from itertools import dropwhile, takewhile
import os
from scipy.optimize import curve_fit
from scipy.interpolate import interp1d
import sklearn.cluster
from scipy.signal import savgol_filter
from scipy import interpolate
from scipy.interpolate import InterpolatedUnivariateSpline
import skimage
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io
import matplotlib.pyplot as plt
import argparse
import tools21cm as t2c
rcParams['font.family'] = 'serif'
rcParams['font.sans-serif'] = ['Computer Modern']
import scipy.stats as st
import brewer2mpl
from scipy.stats import norm
from sklearn.neighbors import KernelDensity
from scipy.spatial.distance import pdist, squareform
from scipy.spatial import Voronoi, Delaunay, delaunay_plot_2d,voronoi_plot_2d
from scipy.spatial import ConvexHull
from scipy.spatial import cKDTree
from random import randrange


def voronoi_volumes(v):
    #v = Voronoi(points,qhull_options='QJ')
    vol = np.zeros(v.npoints)
    #regions, vertices = voronoi_finite_polygons_2d(v)
    #print regions, v.regions
    for i, reg_num in enumerate(v.point_region):

        indices = v.regions[reg_num]
        if -1 in indices: # some regions can be opened
            vol[i] = 1e10
        else:
            vol[i] = ConvexHull(v.vertices[indices],qhull_options='QJ').volume
            #print vol[i]
    return vol

def triangle_area(a,b,c):
    x1, y1 = a
    x2, y2 = b
    x3, y3 = c 
    return abs(0.5 * (((x2-x1)*(y3-y1))-((x3-x1)*(y2-y1))))


def tetravol(a,b,c):
 '''Calculates the volume of a tetrahedron, given vertices a,b,c and d (triplets)'''
 #tetravol=abs(np.dot((a-d),np.cross((b-d),(c-d))))/6
 s = (a+b+c)/2
 area = triangle_area(a,b,c)
 return area

def vol(vor,p):
 '''Calculate volume of 3d Voronoi cell based on point p. Voronoi diagram is passed in v.'''
 dpoints=[]
 vol=0
 for v in vor.regions[vor.point_region[p]]:
  dpoints.append(list(vor.vertices[v]))
 tri=Delaunay(np.array(dpoints))
 for simplex in tri.simplices:
  vol+=tetravol(np.array(dpoints[simplex[0]]),np.array(dpoints[simplex[1]]),np.array(dpoints[simplex[2]]))
 return vol

def voronoi_finite_polygons_2d(vor, radius=None):
    """
    Reconstruct infinite voronoi regions in a 2D diagram to finite
    regions.

    Parameters
    ----------
    vor : Voronoi
        Input diagram
    radius : float, optional
        Distance to 'points at infinity'.

    Returns
    -------
    regions : list of tuples
        Indices of vertices in each revised Voronoi regions.
    vertices : list of tuples
        Coordinates for revised Voronoi vertices. Same as coordinates
        of input vertices, with 'points at infinity' appended to the
        end.

    """

    if vor.points.shape[1] != 2:
        raise ValueError("Requires 2D input")

    new_regions = []
    new_vertices = vor.vertices.tolist()

    center = vor.points.mean(axis=0)
    if radius is None:
        radius = vor.points.ptp().max()

    # Construct a map containing all ridges for a given point
    all_ridges = {}
    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):
        all_ridges.setdefault(p1, []).append((p2, v1, v2))
        all_ridges.setdefault(p2, []).append((p1, v1, v2))

    # Reconstruct infinite regions
    for p1, region in enumerate(vor.point_region):
        vertices = vor.regions[region]
        print vertices
        if all(v >= 0 for v in vertices):
            # finite region
            new_regions.append(vertices)
            continue

        # reconstruct a non-finite region
        ridges = all_ridges[p1]
        new_region = [v for v in vertices if v >= 0]

        for p2, v1, v2 in ridges:
            if v2 < 0:
                v1, v2 = v2, v1
            if v1 >= 0:
                # finite ridge: already in the region
                continue

            # Compute the missing endpoint of an infinite ridge

            t = vor.points[p2] - vor.points[p1] # tangent
            t /= np.linalg.norm(t)
            n = np.array([-t[1], t[0]])  # normal

            midpoint = vor.points[[p1, p2]].mean(axis=0)
            direction = np.sign(np.dot(midpoint - center, n)) * n
            far_point = vor.vertices[v2] + direction * radius

            new_region.append(len(new_vertices))
            new_vertices.append(far_point.tolist())

        # sort region counterclockwise
        vs = np.asarray([new_vertices[v] for v in new_region])
        c = vs.mean(axis=0)
        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])
        new_region = np.array(new_region)[np.argsort(angles)]

        # finish
        new_regions.append(new_region.tolist())

    return new_regions, np.asarray(new_vertices)


def distcorr(X, Y):
    """ Compute the distance correlation function

    >>> a = [1,2,3,4,5]
    >>> b = np.array([1,2,9,4,4])
    >>> distcorr(a, b)
    0.762676242417
    """
    X = np.atleast_1d(X)
    Y = np.atleast_1d(Y)
    if np.prod(X.shape) == len(X):
        X = X[:, None]
    if np.prod(Y.shape) == len(Y):
        Y = Y[:, None]
    X = np.atleast_2d(X)
    Y = np.atleast_2d(Y)
    n = X.shape[0]
    if Y.shape[0] != X.shape[0]:
        raise ValueError('Number of samples must match')
    a = squareform(pdist(X))
    b = squareform(pdist(Y))
    A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()
    B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()

    dcov2_xy = (A * B).sum()/float(n * n)
    dcov2_xx = (A * A).sum()/float(n * n)
    dcov2_yy = (B * B).sum()/float(n * n)
    dcor = np.sqrt(dcov2_xy)/np.sqrt(np.sqrt(dcov2_xx) * np.sqrt(dcov2_yy))
    return dcor

def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    return array[idx]


def load_binary_data(filename, dtype=np.float32): 
    """ 
    We assume that the data was written 
    with write_binary_data() (little endian). 
    """ 
    f = open(filename, "rb") 
    data = f.read() 
    f.close() 
    _data = np.fromstring(data, dtype) 

    return _data




#TEST VORONOI DISTRIB 
dis1 = np.random.normal(loc=0.0, scale=1.0, size=10)
dis2 = np.random.normal(loc=0.0, scale=0.5, size=10)
mean = [0, 0]
cov = [[1, 0], [0, 5]]  # diagonal covariance
dis1 = np.random.multivariate_normal(mean, cov, 10)
cov = [[1, 0], [0, 5]]  # diagonal covariance
dis2 = np.random.multivariate_normal(mean, cov, 20)

vor = Voronoi(dis1)
voronoi_plot_2d(vor)
vor2 = Voronoi(dis2)
voronoi_plot_2d(vor2)

dell = Delaunay(dis1)
delaunay_plot_2d(dell)
dell2 = Delaunay(dis2)
delaunay_plot_2d(dell2)

newr, newv =  voronoi_finite_polygons_2d(vor, radius=None)
print "\n\n"
oldr = []
oldr = [x for x in  vor.regions if x != []]
for p1 in range(len(vor.points)):
    print oldr[p1]
    print newr[p1]
    print


# TEST 2D VORONOI


alltot = []
for ite in range(1):
    mean = [5, 0]
    cov = [[1, 0], [0, 5]]  # diagonal covariance
    dis1 = np.random.multivariate_normal(mean, cov, 20)

    mean = [5, 0]
    cov = [[1, 0], [0, 5]]  # diagonal covariance
    dis2 = np.random.multivariate_normal(mean, cov, 20)
    #print dis2[:,0]
    vor = Voronoi(dis1,qhull_options='Qs')
    vor2 = Voronoi(dis2,qhull_options='Qs')
    voronoi_plot_2d(vor)
    voronoi_plot_2d(vor2)
    #plt.plot(dis1[:,0],dis1[:,1], 'xr', markersize = 5)
    #print vor.points
    #print vor.regions
    voronoi_kdtree = cKDTree(dis2)

    test_point_dist, test_point_regions = voronoi_kdtree.query(dis1, k=1)


    vtot1 = 0
    vtot2 = 0
    for i,p in enumerate(vor.points):
        out=False
        for v in vor.regions[vor.point_region[i]]:
            if v<=-1: #a point index of -1 is returned if the vertex is outside the Vornoi diagram, in this application these should be ignorable edge-cases
                out=True
        if not out:
            pvol=vol(vor,i)
            vtot1+=pvol

    for i,p in enumerate(vor2.points):
        out=False
        for v in vor2.regions[vor2.point_region[i]]:
            if v<=-1: #a point index of -1 is returned if the vertex is outside the Vornoi diagram, in this application these should be ignorable edge-cases
                out=True
        if not out:
            pvol=vol(vor2,i)
            #print pvol
            vtot2+=pvol

    #p1 =  vol(vor,-2)#/vtot1
    #p2 =  vol(vor2,test_point_regions[-2])#/vtot2
    #print vtot1, vtot2

    totp = 0
    nval = 0
    for i,p in enumerate(vor.points):
        out=False
        for v in vor.regions[vor.point_region[i]]:
            if v<=-1: #a point index of -1 is returned if the vertex is outside the Vornoi diagram, in this application these should be ignorable edge-cases
                out=True
        if not out:
            out = False
            for v in vor2.regions[vor2.point_region[test_point_regions[i]]]:
                if v<=-1: #a point index of -1 is returned if the vertex is outside the Vornoi diagram, in this application these should be ignorable edge-cases
                    out=True
            if not out:  
                nval+=1
                lamb =(vol(vor2,test_point_regions[i]))/(vol(vor,i)) #(vol(vor2,test_point_regions[i])/vtot2/len(vor2.points))/(vol(vor,i)/vtot1/len(vor.points))
                pvol = np.log(scipy.stats.poisson.pmf(1,lamb)/scipy.stats.poisson.pmf(1,1))
                print pvol
                totp+=pvol
    alltot.append(totp)
    print len(dis1[:]),nval,totp, np.log(totp)
    
print alltot, np.median(alltot), np.std(alltot)
print vor.vertices


#Data 
data_all = []
vol_lim = 10
z_arr = [10,9,8]
Directory = '21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_new/'
inprefix = 'obs_hii_z'
multi = 1
for ii in range(len(z_arr)):
    name =Directory+inprefix+'%1.6f'%z_arr[ii]

    f = open(name+'_vol.bin', "r")
    volume = np.fromfile(f, dtype=np.uint64)*2
    nbubbles = len(volume)
    print min(volume)
    f = open(name+'_elong.bin', "r")
    all_e = np.fromfile(f, dtype=np.float)

    f = open(name+'_flat.bin', "r")
    all_f = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_spars.bin', "r")
    all_s = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_ncomp.bin', "r")
    all_n = np.fromfile(f, dtype=np.float)

    good = volume > vol_lim
    volume = volume[good]
    print len(volume)
    all_e = all_e[good]
    all_f = all_f[good]
    all_s = all_s[good]
    all_n = all_n[good]

    data_all.append( zip(np.log10(volume), all_e, all_f, all_s, all_n) )
    #data_all.append(zip(np.log10(volume), np.ones(len(volume))))


#  Models

# Diff universe

Directory= '21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_200_univmcmc/'#
#z_arr = [10]
inprefix = 'obs_hii_z'

model_samepar= []
for ii in range(len(z_arr)):
    name =Directory+inprefix+'%1.6f'%z_arr[ii]

    f = open(name+'_vol.bin', "r")
    volume = np.fromfile(f, dtype=np.uint64)*2
    nbubbles = len(volume)
    f = open(name+'_elong.bin', "r")
    all_e = np.fromfile(f, dtype=np.float)

    f = open(name+'_flat.bin', "r")
    all_f = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_spars.bin', "r")
    all_s = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_ncomp.bin', "r")
    all_n = np.fromfile(f, dtype=np.float)

    good = volume > vol_lim
    volume = volume[good]*multi
    all_e = all_e[good]
    all_f = all_f[good]
    all_s = all_s[good]
    all_n = all_n[good]
    print len(all_e)
    #model =  zip(np.log10(volume), all_s, all_n, all_e, all_f) 
    model =   zip(np.log10(volume), all_e, all_f, all_s, all_n)#/stdall
    model_samepar.append(model)
    
#zip(all_s, all_n, all_e, all_f)
#model_15_z9 = zip(volume/np.sum(volume), all_s, all_e, all_f)

# seen = set()
# uniq2 = []
# for x in model_15_z9:
#     if x not in seen:
#         uniq2.append(x)
#         seen.add(x)

# model_15_z9=list(uniq2)
# model_15_z9.extend([[-1,lim1,lim1,lim1],[-1,lim2,lim1,lim1],[-1,lim1,lim1,lim2],[-1,lim1,lim2,lim1]])
# model_15_z9.extend([[-1,lim2,lim2,lim1],[-1,lim2,lim2,lim2],[-1,lim1,lim2,lim2], [-1,lim2,lim1,lim2]])
# model_15_z9.extend([[16,lim1,lim1,lim1],[16,lim2,lim1,lim1], [16,lim2,lim2,lim1],[16,lim2,lim2,lim2],[16,lim1,lim2,lim2],\
#              [16,lim1,lim1,lim2],[16,lim2,lim1,lim2],[16,lim1,lim2,lim1]])


#model_15_z9_vor = Voronoi(model_15_z9,qhull_options='QJ')
#model_15_z9_vol = voronoi_volumes(model_15_z9_vor)

##### Diff R

Directory= '21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_15_22_451/'
inprefix = 'obs_hii_z'
model_r8 = []
for ii in range(len(z_arr)):
    name =Directory+inprefix+'%1.6f'%z_arr[ii]

    f = open(name+'_vol.bin', "r")
    volume = np.fromfile(f, dtype=np.uint64)*2
    nbubbles = len(volume)
    f = open(name+'_elong.bin', "r")
    all_e = np.fromfile(f, dtype=np.float)

    f = open(name+'_flat.bin', "r")
    all_f = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_spars.bin', "r")
    all_s = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_ncomp.bin', "r")
    all_n = np.fromfile(f, dtype=np.float)

    good = volume > vol_lim
    volume = volume[good]*multi
    all_e = all_e[good]
    all_f = all_f[good]
    all_s = all_s[good]
    all_n = all_n[good]
    #model =  zip(np.log10(volume), np.ones(len(volume)))
    model = zip(np.log10(volume), all_e, all_f, all_s, all_n)#zip(all_n,np.ones(len(volume)))#zip(volume, all_s, all_n, all_e, all_f)/stdall
    model_r8.append(model)
    
    
#zip( all_s,all_n, all_e, all_f)#zip(volume, all_s, all_e, all_f)
#model_15_z10 = zip(volume/np.sum(volume), all_s, all_e, all_f)

# seen = set()
# uniq2 = []
# for x in model_15_z10:
#     if x not in seen:
#         uniq2.append(x)
#         seen.add(x)

# model_15_z10 = list(uniq2)
# model_15_z10.extend([[-1,lim1,lim1,lim1],[-1,lim2,lim1,lim1],[-1,lim1,lim1,lim2],[-1,lim1,lim2,lim1]])
# model_15_z10.extend([[-1,lim2,lim2,lim1],[-1,lim2,lim2,lim2],[-1,lim1,lim2,lim2], [-1,lim2,lim1,lim2]])
# model_15_z10.extend([[16,lim1,lim1,lim1],[16,lim2,lim1,lim1], [16,lim2,lim2,lim1],[16,lim2,lim2,lim2],[16,lim1,lim2,lim2],\
#              [16,lim1,lim1,lim2],[16,lim2,lim1,lim2],[16,lim1,lim2,lim1]])

#model_15_z10_vor = Voronoi(model_15_z10,qhull_options='QJ')
#model_15_z10_vol = voronoi_volumes(model_15_z10_vor)

#Diff redshift 8


Directory= '21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_new_R22/'
model_r25 = []

for ii in range(len(z_arr)):
    name =Directory+inprefix+'%1.6f'%z_arr[ii]

    f = open(name+'_vol.bin', "r")
    volume = np.fromfile(f, dtype=np.uint64)*2
    nbubbles = len(volume)
    f = open(name+'_elong.bin', "r")
    all_e = np.fromfile(f, dtype=np.float)

    f = open(name+'_flat.bin', "r")
    all_f = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_spars.bin', "r")
    all_s = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_ncomp.bin', "r")
    all_n = np.fromfile(f, dtype=np.float)

    good = volume > vol_lim
    volume = volume[good]*multi
    all_e = all_e[good]
    all_f = all_f[good]
    all_s = all_s[good]
    all_n = all_n[good]
    #model = zip(np.log10(volume), np.ones(len(volume)))
    model = zip(np.log10(volume), all_e, all_f, all_s, all_n)#zip(all_n,np.ones(len(volume))) zip(volume, all_s, all_n, all_e, all_f)/stdall#zip(all_s, all_n,all_e, all_f)#zip(volume, all_s, all_e, all_f)
    model_r25.append(model)
    
    
    #model_15_z8 = zip(volume/np.sum(volume), all_s, all_e, all_f)

# seen = set()
# uniq2 = []
# for x in model_15_z8:
#     if x not in seen:
#         uniq2.append(x)
#         seen.add(x)

# model_15_z8 = list(uniq2)
# model_15_z8.extend([[-1,lim1,lim1,lim1],[-1,lim2,lim1,lim1],[-1,lim1,lim1,lim2],[-1,lim1,lim2,lim1]])
# model_15_z8.extend([[-1,lim2,lim2,lim1],[-1,lim2,lim2,lim2],[-1,lim1,lim2,lim2], [-1,lim2,lim1,lim2]])
# model_15_z8.extend([[16,lim1,lim1,lim1],[16,lim2,lim1,lim1], [16,lim2,lim2,lim1],[16,lim2,lim2,lim2],[16,lim1,lim2,lim2],\
#              [16,lim1,lim1,lim2],[16,lim2,lim1,lim2],[16,lim1,lim2,lim1]])

#model_15_z8_vor = Voronoi(model_15_z8,qhull_options='QJ')
#model_15_z8_vol = voronoi_volumes(model_15_z8_vor)
                       
# Diff R 25

Directory= '21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_univmcmc/'
model_e97 = []

for ii in range(len(z_arr)):
    name =Directory+inprefix+'%1.6f'%z_arr[ii]

    f = open(name+'_vol.bin', "r")
    volume = np.fromfile(f, dtype=np.uint64)*2
    nbubbles = len(volume)
    f = open(name+'_elong.bin', "r")
    all_e = np.fromfile(f, dtype=np.float)

    f = open(name+'_flat.bin', "r")
    all_f = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_spars.bin', "r")
    all_s = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_ncomp.bin', "r")
    all_n = np.fromfile(f, dtype=np.float)

    good = volume > vol_lim
    volume = volume[good]*multi
    all_e = all_e[good]
    all_f = all_f[good]
    all_s = all_s[good]
    all_n = all_n[good]
    print len(all_e)
    #model =  zip(np.log10(volume), np.ones(len(volume)))
    model = zip(np.log10(volume), all_e, all_f, all_s, all_n)#zip(all_n,np.ones(len(volume)))#zip(volume, all_s, all_n, all_e, all_f)/stdall#zip(all_s, all_n,all_e, all_f)#zip(volume, all_s, all_e, all_f)
    model_e97.append(model)

    
    #model_25_z9 = zip(volume/np.sum(volume), all_s, all_e, all_f)

# seen = set()
# uniq2 = []
# for x in model_25_z9:
#     if x not in seen:
#         uniq2.append(x)
#         seen.add(x)

# model_25_z9=list(uniq2)
# model_25_z9.extend([[-1,lim1,lim1,lim1],[-1,lim2,lim1,lim1],[-1,lim1,lim1,lim2],[-1,lim1,lim2,lim1]])
# model_25_z9.extend([[-1,lim2,lim2,lim1],[-1,lim2,lim2,lim2],[-1,lim1,lim2,lim2], [-1,lim2,lim1,lim2]])
# model_25_z9.extend([[16,lim1,lim1,lim1],[16,lim2,lim1,lim1], [16,lim2,lim2,lim1],[16,lim2,lim2,lim2],[16,lim1,lim2,lim2],\
#              [16,lim1,lim1,lim2],[16,lim2,lim1,lim2],[16,lim1,lim2,lim1]])

#model_25_z9_vor = Voronoi(model_25_z9,qhull_options='QJ')
#model_25_z9_vol = voronoi_volumes(model_25_z9_vor)

# Diff R 8

Directory= '21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_58_15_487/'
model_e160 = []

for ii in range(len(z_arr)):
    name =Directory+inprefix+'%1.6f'%z_arr[ii]

    f = open(name+'_vol.bin', "r")
    volume = np.fromfile(f, dtype=np.uint64)*2
    nbubbles = len(volume)
    f = open(name+'_elong.bin', "r")
    all_e = np.fromfile(f, dtype=np.float)

    f = open(name+'_flat.bin', "r")
    all_f = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_spars.bin', "r")
    all_s = np.fromfile(f, dtype=np.float)
    
    f = open(name+'_ncomp.bin', "r")
    all_n = np.fromfile(f, dtype=np.float)

    good = volume > vol_lim
    volume = volume[good]*multi
    all_e = all_e[good]
    all_f = all_f[good]
    all_s = all_s[good]
    all_n = all_n[good]
    #model =  zip(np.log10(volume), np.ones(len(volume)))
    model =  zip(np.log10(volume), all_e, all_f, all_s, all_n)#zip(all_n,np.ones(len(volume)))##zip(all_s,all_n, all_e, all_f)# zip(volume, all_s, all_e, all_f)
    model_e160.append(model)
    #model_8_z9 = zip(volume/np.sum(volume), all_s, all_e, all_f)

# seen = set()
# uniq2 = []
# for x in model_8_z9:
#     if x not in seen:
#         uniq2.append(x)
#         seen.add(x)

# model_8_z9=list(uniq2)
# model_8_z9.extend([[-1,lim1,lim1,lim1],[-1,lim2,lim1,lim1],[-1,lim1,lim1,lim2],[-1,lim1,lim2,lim1]])
# model_8_z9.extend([[-1,lim2,lim2,lim1],[-1,lim2,lim2,lim2],[-1,lim1,lim2,lim2], [-1,lim2,lim1,lim2]])
# model_8_z9.extend([[16,lim1,lim1,lim1],[16,lim2,lim1,lim1], [16,lim2,lim2,lim1],[16,lim2,lim2,lim2],[16,lim1,lim2,lim2],\
#              [16,lim1,lim1,lim2],[16,lim2,lim1,lim2],[16,lim1,lim2,lim1]])

#model_8_z9_vor = Voronoi(model_8_z9,qhull_options='QJ')
#model_8_z9_vol = voronoi_volumes(model_8_z9_vor)




model_dis = [model_samepar,model_r8,model_r25,model_e97,model_e160]
#model_vor = [model_15_z9_vor,model_15_z10_vor,model_15_z8_vor,model_25_z9_vor,model_8_z9_vor]
#model_vol = [model_15_z9_vol,model_15_z10_vol,model_15_z8_vol,model_25_z9_vol,model_8_z9_vol]
name_case = ["Same parameters", "R = 8 Mpc", "R = 25 Mpc", "{97,6,5}", "{160,5,5.2}"]


print scipy.stats.poisson.pmf(27638,24354)
print scipy.stats.poisson.logpmf(27638,27638)
print scipy.stats.poisson.pmf(725,645)
print scipy.stats.poisson.logpmf(725,725)

print (27638-24354)/27638.
print (725-645)/725.



#### METHOD FINAL : Euclidian distance of the z-score normalization

#METHOD 3, drawing data from model, with single points
lbd = 0.001

Lfinal = []
Poissfinal = []
Euclidfinal = []
Euclidnormfinal = []


for j in range(len(model_dis)):
    casedis = model_dis[j]
    print "CASE %d: %s" %(j, name_case[j])
    allz_eucl = 0
    allz_euclf = 0
    allz_maha = 0
    allz_poiss = 0
    for jj in range(len(z_arr)):
        curdata = data_all[jj]
        stdall = np.std(curdata, axis=0)
        #print stdall
       # stdall[1] = 1.
        curdis = casedis[jj]/stdall
        #print curdis[:,0]
        voronoi_kdtree = cKDTree(curdis)
        size_data = len(curdata[:])
        size_model = len(curdis[:])
        cov = np.cov(np.asarray(curdata/stdall).T)

        print size_data, size_model

        eucl_tot = 0
        maha_tot = 0
        nval = 0

        for i in range(size_data):
            vor_pt = np.asarray(curdata[i])/stdall
            test_point_dist, test_point_regions = voronoi_kdtree.query(vor_pt, k=1)
            #d = find_nearest(curdis[:,0], vor_pt[0])
            model_pt = np.asarray(curdis[test_point_regions]) #find_nearest(curdis, vor_pt)#
            nval+=1            
            #eucl_norm = 0.5*(np.std(vor_pt-model_pt)**2) / (np.std(vor_pt)**2+np.std(model_pt)**2)
            maha = scipy.spatial.distance.mahalanobis(vor_pt,model_pt,np.linalg.inv(cov))
            #eucl = (d-vor_pt[0])**2
            #eucl_tot += 1/test_point_dist**5
            maha_tot += maha**5#1/(maha**5+1) 
        #print maha_tot, eucl_tot
        
        poisson = scipy.stats.poisson.logpmf(size_data,size_model)
        like_eucl = +lbd * float(size_data)/size_model*eucl_tot #float(size_model)/size_data*
        like_maha = -lbd * size_model/(float(size_data))*maha_tot
        allz_eucl += like_eucl 
        allz_maha += like_maha 
        allz_poiss+= poisson
        print  "z = %d"%z_arr[jj]
        print "Number of values combined: %ld"%nval
        print "Distance eucl %f  ---  Distance Maha %f"%(eucl_tot, maha_tot)
        print "Value of the Poisson distribution: %f"%poisson
        print "Likelihood at z (eucl) : %f ----- (maha) : %f"%(like_eucl, like_maha)#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
        print
    print " *********************** "
    print  "FINAL LIKELIHOOD z = [10,9,8]"
    #print "Number of values combined: %ld"%nval
    print "Distance eucl %f  ---  Distance Maha %f"%(allz_eucl, allz_maha)
    print "Value of the Poisson distribution: %f"%allz_poiss
    print "Likelihood at z (eucl) : %f ----- (maha) : %f "%(allz_eucl+allz_poiss, allz_maha+allz_poiss)#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print " *********************** "

    print 
    print
#print nval,totp, np.log(scipy.stats.poisson.pmf(len(dis1[:]),len(dis2[:])))
    


#### PLOTTING METHOD FINAL : Euclidian distance of the z-score normalization, plottting

#METHOD 3, drawing data from model, with single points
lbd = [0.001]

Lfinal = []
Poissfinal = []
Euclidfinal = []
Euclidnormfinal = []


for j in range(len(model_dis)):
    casedis = model_dis[j]
    print "CASE %d: %s" %(j, name_case[j])
    allz_eucl = 0
    allz_euclf = 0
    allz_maha = 0
    allz_poiss = 0
    for jj in range(len(z_arr)):
        
        curdata = data_all[jj]
        curdis = casedis[jj]
        curdata = [curdata[i] for i in range(len(curdata[:])) if curdata[i][0] > 0]
        curdis = [curdis[i] for i in range(len(curdis[:])) if curdis[i][0] > 0]
        #print curdis
        stdall = np.std(curdata, axis=0)
        curdis = curdis/stdall
        voronoi_kdtree = cKDTree(curdis)
        size_data = len(curdata[:])
        size_model = len(curdis[:])
    
        print size_data, size_model

        eucl_tot = 0
        maha_tot = 0
        nval = 0


        for i in range(size_data):
            vor_pt = np.asarray(curdata[i])/stdall
            test_point_dist, test_point_regions = voronoi_kdtree.query(vor_pt, k=1)
            model_pt = np.asarray(curdis[test_point_regions])
            nval+=1            
            #eucl_norm = 0.5*(np.std(vor_pt-model_pt)**2) / (np.std(vor_pt)**2+np.std(model_pt)**2)
            cov = np.cov(np.asarray(curdata/stdall).T)
            maha = scipy.spatial.distance.mahalanobis(vor_pt,model_pt,np.linalg.inv(cov))
            eucl_tot += test_point_dist**5
            maha_tot += maha**5
        
        poisson = scipy.stats.poisson.logpmf(size_data,size_model)
        allz_poiss += poisson
        allz_eucl  += maha_tot#eucl_tot
        allz_euclf +=  float(size_model)/size_data*maha_tot#eucl_tot
        
        #like_eucl = float(size_model)/size_data*eucl_tot+poisson #float(size_model)/size_data*
        #like_maha = -lbd * float(size_model)/size_data*maha_tot+poisson
        #allz_eucl += lbd *float(size_model)/size_data*eucl_tot
        #allz_maha += lbd *float(size_model)/size_data*maha_tot
#         print  "z = %d"%z_arr[jj]
#         print "Number of values combined: %ld"%nval
#         print "Distance eucl %f  ---  Distance Maha %f"%(eucl_tot, maha_tot)
#         print "Value of the Poisson distribution: %f"%poisson
        #print "Likelihood at z (eucl) : %f ----- (maha) : %f"%(like_eucl, like_maha)#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
        #print
    print " *********************** "
    print  "FINAL LIKELIHOOD z = [10,9,8]"
    #print "Number of values combined: %ld"%nval
    print "Distance eucl %f  ---  Distance eucl norm %f"%(allz_eucl, allz_euclf)
    print "Value of the Poisson distribution: %f"%allz_poiss
    print "Likelihood at z (eucl) : %f ----- (eucl norm) : %f "%(-allz_eucl+allz_poiss, -allz_euclf+allz_poiss)#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print " *********************** "
    Poissfinal.append(allz_poiss)
    Euclidfinal.append(allz_eucl)
    Euclidnormfinal.append(allz_euclf)

    print 
    print
#print nval,totp, np.log(scipy.stats.poisson.pmf(len(dis1[:]),len(dis2[:])))
    


lbd = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]
volume = [0,10,100,1000,10000,100000]
like1 = [[] for i in range(len(model_dis))]
like2 = [[] for i in range(len(model_dis))]
like3 = [[] for i in range(len(model_dis))]
like4 = [[] for i in range(len(model_dis))]
like5 = [[] for i in range(len(model_dis))]
like6 = [[] for i in range(len(model_dis))]


#Volume 10



# for i in range(len(model_dis)):
#     for j in range(len(lbd)):
#         like1[i].append(-(Poissfinal[i]-lbd[j]*Euclidfinal[i]))
#         like2[i].append(-(Poissfinal[i]-lbd[j]*Euclidnormfinal[i]))

for i in range(len(model_dis)):
    like1[i].append(-(Poissfinal[i]-Euclidfinal[i]))
    like2[i].append(-(Poissfinal[i]-Euclidnormfinal[i]))
    like3[i].append(Poissfinal[i])
    like4[i].append(Poissfinal[i])
    like5[i].append(Euclidfinal[i])
    like6[i].append(Euclidnormfinal[i])



#print like1
lbd = [1e-8,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]

like1 = np.zeros((len(model_dis), len(lbd)))
like2 = np.zeros((len(model_dis), len(lbd)))

for i in range(len(model_dis)):
    for j in range(len(lbd)):
        like1[i][j] = Poissfinal[i]-lbd[j]*Euclidfinal[i]
        like2[i][j] = Poissfinal[i]-lbd[j]*Euclidnormfinal[i]


fig = plt.figure(figsize=(30,15))

ax = plt.subplot(2, 1, 1)
ax = plt.gca()
ax.set_xscale('log')
#ax.set_yscale('log')
for i in range(len(model_dis)):
    if(i == 0):
        plt.plot(lbd, like1[i]/like1[0], linewidth=5.0, c = 'k')
    else:
        plt.plot(lbd, like1[i]/like1[0], linewidth=3.0)

#plt.xlabel(r'$\lambda$', fontsize=20)
plt.xlabel(r'Lambda', fontsize=20)
#plt.ylim(0,5)
#plt.ylabel(r'Log(L)$_{norm}$ = $\frac{(Poisson - \lambda \sum(d^2))}{log(L)_{ref}}$', fontsize=20)
#plt.ylabel(r'Log(L)$_{norm}$ = Poisson', fontsize=20)
plt.ylabel(r'Log(L)$_{norm}$ = $\frac{ \sum(d^2))}{log(L)_{ref}}$', fontsize=20)
plt.tick_params(axis = 'both', which = 'both', labelsize = 24)
#plt.title("ONLY Ncompactness")
ax.legend(labels = name_case,  fontsize=20, frameon=False )

ax = plt.subplot(2, 1, 2)
ax = plt.gca()
ax.set_xscale('log')
#ax.set_yscale('log')
for i in range(len(model_dis)):
    if(i == 0):
        plt.plot(lbd, like2[i]/like2[0], linewidth=5.0, c = 'k')
    else:
        plt.plot(lbd, like2[i]/like2[0], linewidth=3.0)


#plt.xlabel(r'$\lambda$', fontsize=20)
plt.xlabel(r'Volume limite', fontsize=20)
#plt.ylim(0,5)
plt.title("ONLY Ncompactness")

#plt.ylabel(r'Log(L)$_{norm}$ = $\frac{(Poisson - \lambda \frac{Nm}{Nd} \sum(d^2))}{log(L)_{ref}}$', fontsize=20)
plt.ylabel(r'Log(L)$_{norm}$ = $\frac{ \frac{Nm}{Nd} \sum(d^2))}{log(L)_{ref}}$', fontsize=20)

plt.tick_params(axis = 'both', which = 'both', labelsize = 24)

ax.legend(labels = name_case,  fontsize=20, frameon=False )


#print like1
fig = plt.figure(figsize=(30,15))

ax = plt.subplot(2, 1, 1)
ax = plt.gca()
ax.set_xscale('log')
#ax.set_yscale('log')
volume = [1,10,100,1000,10000]
for i in range(len(model_dis)):
    if(i == 0):
        plt.plot(volume, np.asarray(like5[i])/np.asarray(like5[0]), linewidth=5.0, c = 'k')
    else:
        plt.plot(volume, np.asarray(like5[i])/np.asarray(like5[0]), linewidth=3.0)

#plt.xlabel(r'$\lambda$', fontsize=20)
plt.xlabel(r'Volume limit', fontsize=20)
#plt.ylim(0,5)
#plt.ylabel(r'Log(L)$_{norm}$ = $\frac{(Poisson - \lambda \sum(d^2))}{log(L)_{ref}}$', fontsize=20)
#plt.ylabel(r'Log(L)$_{norm}$ = Poisson', fontsize=20)
plt.ylabel(r'Log(L)$_{norm}$ = $\frac{ \sum(d^2))}{log(L)_{ref}}$', fontsize=20)
plt.tick_params(axis = 'both', which = 'both', labelsize = 24)
plt.title("ONLY Ncompactness")
ax.legend(labels = name_case,  fontsize=20, frameon=False )

ax = plt.subplot(2, 1, 2)
ax = plt.gca()
ax.set_xscale('log')
#ax.set_yscale('log')
for i in range(len(model_dis)):
    if(i == 0):
        plt.plot(volume, np.asarray(like6[i])/np.asarray(like6[0]), linewidth=5.0, c= 'k')
    else:
        plt.plot(volume, np.asarray(like6[i])/np.asarray(like6[0]), linewidth=3.0)


#plt.xlabel(r'$\lambda$', fontsize=20)
plt.xlabel(r'Volume limite', fontsize=20)
#plt.ylim(0,5)
plt.title("ONLY Ncompactness")

#plt.ylabel(r'Log(L)$_{norm}$ = $\frac{(Poisson - \lambda \frac{Nm}{Nd} \sum(d^2))}{log(L)_{ref}}$', fontsize=20)
plt.ylabel(r'Log(L)$_{norm}$ = $\frac{ \frac{Nm}{Nd} \sum(d^2))}{log(L)_{ref}}$', fontsize=20)

plt.tick_params(axis = 'both', which = 'both', labelsize = 24)

ax.legend(labels = name_case,  fontsize=20, frameon=False )


#METHOD 1a: Probability is ratio of volumes, assuming data is drawn from model

for j in range(len(model_vol)):
    curdis = model_dis[j]
    curvor = model_vor[j]
    curvol = model_vol[j]
    voronoi_kdtree = cKDTree(curdis)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0

    for i,p in enumerate(data_vor.points):
    
        test_point_dist, test_point_regions = voronoi_kdtree.query(data_vor.points[i], k=1)
               
        p2 = 1/(curvol[test_point_regions]*size_model)
        p1 = 1/(data_vol[i]*size_data)

    #TWO POSSIBILITY HERE
        #if vol_2 > vol_1:
        lamb =p1/p2
        #else :
        #    lamb = vol_1/vol_2
        
        pvol = np.log(scipy.stats.poisson.pmf(1,lamb))
        if pvol == float("-inf"):
            pvol = -100

        nval+=1
        totp+=pvol
        
    poisson = np.log(scipy.stats.poisson.pmf(size_data,size_model)) 
    like = totp+poisson
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 


#METHOD 1b: Probability is ratio of volumes, assuming model is drawn from data
for j in range(len(model_vol)):
    curdis = model_dis[j]
    curvor = model_vor[j]
    curvol = model_vol[j]
    voronoi_kdtree = cKDTree(data)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0

    for i,p in enumerate(curvor.points):

        test_point_dist, test_point_regions = voronoi_kdtree.query(p, k=1)
            
        p2 = 1/(curvol[i]*size_model)
        p1 = 1/(data_vol[test_point_regions]*size_data)

        lamb =p1/p2

        pvol = np.log(scipy.stats.poisson.pmf(1,lamb))
        if pvol == float("-inf"):
            pvol = -100

        nval+=1
        totp+=pvol
        
    poisson = np.log(scipy.stats.poisson.pmf(size_model,size_data)) 
    like = totp+poisson
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 

#print  nval,totp, np.log(scipy.stats.poisson.pmf(len(dis2[:]),len(dis1[:]))) #-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    


#METHOD 1c: Probability is ratio of volumes, assuming model is drawn from data, with a pick and drop random

for j in range(len(model_vol)):
    curdis = model_dis[j]
    curvor = model_vor[j]
    curvol = model_vol[j]
    voronoi_kdtree = cKDTree(data)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0


    for i in range(data_vor.npoints):
        pt =randrange(curvor.npoints)
        test_point_dist, test_point_regions = voronoi_kdtree.query(curvor.points[pt], k=1)
        
        p2 = 1/(curvol[pt]*size_model)
        p1 = 1/(data_vol[test_point_regions]*size_data)
     #   if(p1 >= p2):
        lamb = p1/p2
      #  else :
      #      lamb = p2/p1
        pvol = np.log(scipy.stats.poisson.pmf(1,lamb))
      #  if(vol_2 > 1e20 or vol_1 > 1e20 ):
      #      print "err"
        if pvol == float("-inf"):
            pvol = -1000
        totp+=pvol

        nval+=1
        
    poisson = 0#np.log(scipy.stats.poisson.pmf(size_model-16,size_data-16)) 
    like = totp+poisson
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 

#print  nval,totp, np.log(scipy.stats.poisson.pmf(len(dis2[:]),len(dis1[:]))) #-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    


#METHOD 2a, drawing model from data, with single points


for j in range(len(model_vol)):
    curdis = model_dis[j]
    curvor = model_vor[j]
    curvol = model_vol[j]
    voronoi_kdtree = cKDTree(data)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0


    for i,p in enumerate(curvor.points):
        vor2_pt = p
        test_point_dist, test_point_regions = voronoi_kdtree.query(vor2_pt, k=1)

        nval+=1            
        if((data_vol[test_point_regions]*size_data) >1e50):
            continue
        pvol = 1/(data_vol[test_point_regions]*size_data)
       # print (data_vol[test_point_regions]*size_data)
        if pvol == float("-inf"):
            print "haha "+ str(lamb) 
            continue
        totp+=np.log(pvol)
        
    poisson = np.log(scipy.stats.poisson.pmf(size_model,size_data)) 
    like = totp+poisson
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 

#print nval,totp, np.log(scipy.stats.poisson.pmf(len(dis2[:]),len(dis1[:])))
    


#METHOD 2b, drawing data from model, with single points

for j in range(len(model_vol)):
    curdis = model_dis[j]
    curvor = model_vor[j]
    curvol = model_vol[j]
    voronoi_kdtree = cKDTree(curdis)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0


    for i,p in enumerate(data_vor.points):
        vor_pt = p
        test_point_dist, test_point_regions = voronoi_kdtree.query(vor_pt, k=1)

        nval+=1            
        pvol = 1/(curvol[test_point_regions]*size_model) #*size_model
        #if curvol[test_point_regions] > 1e90:
        #        continue
        if pvol == float("-inf"):
            print "haha "+ str(lamb) 
            continue
        totp+=np.log(pvol)
           # print vol1[test_point_regions],pvol, totp
                #print pvol, totp
    poisson = np.log(scipy.stats.poisson.pmf(size_data,size_model)) 
    like = totp+poisson
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 
    
#print nval,totp, np.log(scipy.stats.poisson.pmf(len(dis1[:]),len(dis2[:])))
    


#METHOD 2c, drawing model from data, with single points, random

for j in range(len(model_vol)):
    curdis = model_dis[j]
    curvor = model_vor[j]
    curvol = model_vol[j]
    voronoi_kdtree = cKDTree(data)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0



#     for i,p in enumerate(curvor.points):
#         vor2_pt = p
#         test_point_dist, test_point_regions = voronoi_kdtree.query(vor2_pt, k=1)

#         nval+=1            
#         if((data_vol[test_point_regions]*size_data) >1e50):
#             continue
#         pvol = 1/(data_vol[test_point_regions]*size_data)
#        # print (data_vol[test_point_regions]*size_data)
#         if pvol == float("-inf"):
#             print "haha "+ str(lamb) 
#             continue
#         totp+=np.log(pvol)
        
#     poisson = np.log(scipy.stats.poisson.pmf(size_model,size_data)) 
#     like = totp+poisson
#     print  name_case[j]
#     print "Number of values combined: %ld"%nval
#     print "Value of the combined Pi %f"%totp
#     print "Value of the Poisson distribution: %f"%poisson
#     print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
#     print 

#print nval,totp, np.log(scipy.stats.poisson.pmf(len(dis2[:]),len(dis1[:])))
    
    
    for i,p in  enumerate(data_vor.points):
        pt =randrange(curvor.npoints-16)
        test_point_dist, test_point_regions = voronoi_kdtree.query(curvor.points[pt], k=1)

        if(data_vol[test_point_regions]>1e50):
            print curvor.points[pt]
            print data_vor.points[test_point_regions]
            continue

        nval+=1            
        pvol = 1/(data_vol[test_point_regions]*size_data)

        totp+=np.log(pvol)
      #  print vol1[test_point_regions],pvol, totp
                #print pvol, totp
    poisson = np.log(scipy.stats.poisson.pmf(size_model,size_data)) 
    like = totp+poisson
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 
    
   # print nval,totp, np.log(scipy.stats.poisson.pmf(vor.npoints,vor2.npoints))
    


#METHOD 3, drawing data from model, with single points
from sklearn import preprocessing

for j in range(len(model_dis)):
    curdis = model_dis[j]
    #curvor = model_vor[j]
    #curvol = model_vol[j]
    voronoi_kdtree = cKDTree(curdis)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0


    for i in range(size_data):
        vor_pt = np.asarray(data[i])
        test_point_dist, test_point_regions = voronoi_kdtree.query(vor_pt, k=1)
        model_pt = np.asarray(curdis[test_point_regions])
        nval+=1            
        summ = 0
       # for jj in range(4):
       #     summ += ((vor_pt[jj]-curdis[test_point_regions][jj])/vor_pt[jj])**2
        #print np.std(vor_pt)**2,np.std(model_pt)**2
        summ = 0.5*(np.std(vor_pt-model_pt)**2) / (np.std(vor_pt)**2+np.std(model_pt)**2)
        eucl = np.linalg.norm(vor_pt-model_pt)
        #cov = np.cov(np.concatenate((np.asarray(data),np.asarray(curdis))).T)
        #print cov
        #tttt = scipy.spatial.distance.mahalanobis(vor_pt,model_pt,np.linalg.inv(cov))
    #summ = np.sum((vec1-vec2)**2)
        distt = summ
       # print distt, test_point_dist
        totp+=test_point_dist
           # print vol1[test_point_regions],pvol, totp
                #print pvol, totp
    poisson = np.log(scipy.stats.poisson.pmf(size_data,size_model)) 
    like = -float(size_model)/size_data*totp+poisson #float(size_model)/size_data*
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 
    
#print nval,totp, np.log(scipy.stats.poisson.pmf(len(dis1[:]),len(dis2[:])))
    


for j in range(len(model_vol)):
    curdis = model_dis[j]
    curvor = model_vor[j]
    curvol = model_vol[j]
    voronoi_kdtree = cKDTree(data)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0


    for i in range(size_model):
        vor_pt = np.array(curdis[i])
        test_point_dist, test_point_regions = voronoi_kdtree.query(vor_pt, k=1)
        model_pt = np.asarray(data[test_point_regions])
        nval+=1            
        summ = 0.5*(np.std(vor_pt-model_pt)**2) / (np.std(vor_pt)**2+np.std(model_pt)**2)

        totp+=summ# np.log(pvol)
           # print vol1[test_point_regions],pvol, totp
                #print pvol, totp
    poisson = np.log(scipy.stats.poisson.pmf(size_data,size_model)) 
    like = -float(size_model)/size_data*totp+poisson
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 
    


for j in range(len(model_vol)):
    curdis = model_dis[j]
    curvor = model_vor[j]
    curvol = model_vol[j]
    voronoi_kdtree = cKDTree(data)
    size_data = len(data[:])
    size_model = len(curdis[:])
    
    print size_data, size_model

    totp = 0
    nval = 0


    for i in range(size_data):
        pt =randrange(size_model-16)
        test_point_dist, test_point_regions = voronoi_kdtree.query(curdis[pt], k=1)
        vor_pt = np.asarray(data[test_point_regions])
        model_pt = np.asarray(curdis[pt])
        nval+=1            
        summ = 0.5*(np.std(vor_pt-model_pt)**2) / (np.std(vor_pt)**2+np.std(model_pt)**2)

        totp+=summ# np.log(pvol)
           # print vol1[test_point_regions],pvol, totp
                #print pvol, totp
    poisson = np.log(scipy.stats.poisson.pmf(size_data,size_model)) 
    like = -float(size_model)/size_data*totp+poisson
    print  name_case[j]
    print "Number of values combined: %ld"%nval
    print "Value of the combined Pi %f"%totp
    print "Value of the Poisson distribution: %f"%poisson
    print "Final Likelihood: %f"%like#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    print 





# Kernel Density estimation

#dis2 = np.random.multivariate_normal(mean, cov, 100)
dis2 = np.asarray(zip(np.log10(volume), all_s, all_n))

print dis2.T
kde = stats.gaussian_kde(dis2.T)
density = kde(dis2.T)
print density
fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))
ax.scatter(dis2.T[0][:],dis2.T[1][:],dis2.T[2][:], c=density)
print kde.evaluate([3,1,1])


z = 10
max_baseline = 2.
output_dtheta  = (1+float(z))*21e-5/max_baseline
output_ang_res = output_dtheta*t2c.cm.z_to_cdist(float(z)) *  ((float(128))/250)
print output_ang_res


# Create Mock tomographic observations / Run disccoman 


z_arr = [10,9,8]

ncells = 128
boxsize = 256
Lfinal = []
Poissfinal = []
Euclidfinal = []
Euclidnormfinal = []
lbd = 0.001
dist = []

#Directory ='Simulations/128_250_U2/30_8_47/'#'21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_15_22_451/'

#Data 
data_all = []
vol_lim = 10
z_arr = [10,9,8]
Directory = '21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_new/'
inprefix = 'obs_hii_z'
multi = 1

LLfinal=-10000
data_all = []
for ii in range(len(z_arr)):
    name =Directory+inprefix+'%1.6f'%z_arr[ii]

    f = open(name+'_vol.bin', "r")
    volume = np.fromfile(f, dtype=np.uint64)*2
    nbubbles = len(volume)
    print min(volume)
    f = open(name+'_elong.bin', "r")
    all_e = np.fromfile(f, dtype=np.float)

    f = open(name+'_flat.bin', "r")
    all_f = np.fromfile(f, dtype=np.float)

    f = open(name+'_spars.bin', "r")
    all_s = np.fromfile(f, dtype=np.float)

    f = open(name+'_ncomp.bin', "r")
    all_n = np.fromfile(f, dtype=np.float)

    good = volume > vol_lim
    volume = volume[good]
    print len(volume)
    all_e = all_e[good]
    all_f = all_f[good]
    all_s = all_s[good]
    all_n = all_n[good]
    data_all.append( zip(np.log10(volume), all_e, all_f, all_s, all_n) )
    #data_all.append(zip(np.log10(volume), np.ones(len(volume))))


while(LLfinal < -200):

    noise_arr= []
    #     for ii in range(len(z_arr)):


    #             z = z_arr[ii]
    #             uv_map = np.loadtxt('21CMMC/21CMMC_Voronoi/Programs/NoiseData/UVmap_SKA/128/uv_map_z%1.6f.txt'%(z), ndmin=2)

    #             noise_arr.append(t2c.noise_cube_coeval(ncells, z, obs_time=1000, boxsize=250, total_int_time=6.,\
    #                                               int_time=10., declination=-30., uv_map=uv_map, N_ant=512, \
    #                                               verbose=False, fft_wrap=False))
    #             hdu = fits.PrimaryHDU(noise_arr[ii])
    #             hdul = fits.HDUList([hdu])
    #             hdul.writeto('21CMMC/21CMMC_Voronoi/Programs/NoiseData/Noise_map_ska/noise_arr_%1.6f_test2.fits'%(z_arr[ii]), overwrite='true')
    # #     #'21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_new/'#    
    Directory ='21CMMC/21CMMC_Voronoi/Programs/MockObs/mock_58_15_487//' #'Simulations/128_250_U2/30_8_47/'
    inprefix = 'obs_hii_z'
    for ii in range(len(z_arr)):
        z = z_arr[ii]
       # print z
       # name = '%s/MockObs/3params_128/NeutralFraction_1.000000_1.000000_%1.6f.txt'%( Directory, z_arr[ii])
       # nfxh =  np.loadtxt('%s'%(name))

        name = '%s/delta_T_1.000000_1.000000_z%1.6f_128_250Mpc'%( Directory, z_arr[ii])
        data = load_binary_data(name)
        data = data.reshape((int(ncells), int(ncells), int(ncells)), order='C')
        data.shape = (int(ncells), int(ncells), int(ncells))

        uv_map = np.loadtxt('21CMMC/21CMMC_Voronoi/Programs/NoiseData/UVmap_SKA/128/uv_map_z%1.6f.txt'%(z), ndmin=2)

        noise = fits.open('21CMMC/21CMMC_Voronoi/Programs/NoiseData/Noise_map_ska/noise_arr_%1.6f.fits'%z)[0].data
        
        #noise_arr[ii]#t2c.noise_cube_coeval(ncells, z, obs_time=1000, boxsize=250, total_int_time=6.,\
                              #            int_time=10., declination=-30., uv_map=uv_map, N_ant=512, \
                              #            verbose=False, fft_wrap=False)

        max_baseline = 2.
        output_dtheta  = (1+float(z))*21e-5/max_baseline
        output_ang_res = output_dtheta*t2c.cm.z_to_cdist(float(z)) \
                         * ncells/(float(boxsize))


        kernel = t2c.gauss_kernel(ncells, sigma=1., fwhm=output_ang_res)
        kernelfft = np.fft.fft2(np.fft.ifftshift(kernel))
        kernelfft[uv_map == 0] = 0
        uv_map = kernelfft

        data_dtb_smo = np.zeros((data.shape))


        for i in range(ncells):                                                 
            data_dtb_smo[:,:,i] =np.real(np.fft.ifft2(np.multiply(np.fft.fft2(data[:,:,i]+noise[:,:,i]), \
                                                                   uv_map/np.max(uv_map))))    

        output_met1 = data_dtb_smo#np.zeros(data.shape)


        kernel = t2c.tophat_kernel(ncells, output_ang_res)
        #kernel = t2c.gauss_kernel(ncells, sigma=1., fwhm=output_ang_res)
        for i in range(ncells):
             output_met1[i,:,:] =t2c.smooth_with_kernel(data_dtb_smo[i,:,:], kernel)


        img = output_met1.astype(float)

        allth=[]
        for i in range(ncells):
            th = skimage.filters.threshold_triangle(img[:,:,i])
            allth.append(th)

        newdata2 = img < np.median(allth)


        curnf = 1. - np.count_nonzero(newdata2.astype(float)) / float(ncells**3)
      #  nfdtb = 1. - np.count_nonzero(data_th.astype(float)) / float(ncells**3)

        hdu = fits.PrimaryHDU(newdata2.astype('uint8'))
        hdul = fits.HDUList([hdu])
        hdul.writeto('%s/obs_z%1.6f.fits'%(Directory,z), overwrite='true')


        command = "mpirun -np 1 ./disccoman/disccoman -g 1,1,1 -l 2 --intype fits \
         --attribute 4 -c 26   --threads 1   -f eor --eor hii   \
         --inprefix %s/obs_z%1.6f --outprefix %s/obs_hii_z%1.6f \
         -v info "%(Directory,z_arr[ii],Directory,z_arr[ii])
        os.system(command)  
    
    model_test = []

    for ii in range(len(z_arr)):
        name =Directory+inprefix+'%1.6f'%z_arr[ii]

        f = open(name+'_vol.bin', "r")
        volume = np.fromfile(f, dtype=np.uint64)*2
        nbubbles = len(volume)
        f = open(name+'_elong.bin', "r")
        all_e = np.fromfile(f, dtype=np.float)

        f = open(name+'_flat.bin', "r")
        all_f = np.fromfile(f, dtype=np.float)

        f = open(name+'_spars.bin', "r")
        all_s = np.fromfile(f, dtype=np.float)

        f = open(name+'_ncomp.bin', "r")
        all_n = np.fromfile(f, dtype=np.float)

        good = volume > 10

        volume = volume[good]

        all_e = all_e[good]
        all_f = all_f[good]
        all_s = all_s[good]
        all_n = all_n[good]
        #model =  zip(np.log10(volume), np.ones(len(volume)))
        model = zip(np.log10(volume), all_e, all_f, all_s, all_n)#zip(all_n,np.ones(len(volume)))#zip(volume, all_s, all_n, all_e, all_f)/stdall#zip(all_s, all_n,all_e, all_f)#zip(volume, all_s, all_e, all_f)
        model_test.append(model)
        

    casedis = model_test
    allz_eucl = 0
    allz_euclf = 0
    allz_maha = 0
    allz_poiss = 0
    for jj in range(len(z_arr)):
        curdata = data_all[jj]
        size_data = len(curdata[:])
        
        stdall = np.std(curdata, axis=0)
        curdis = casedis[jj]
        curdis = curdis/stdall
        size_model = len(curdis[:])

        #curdis = [curdis[i] for i in range(size_model) if curdis[i][0]>=3]
        #size_model2 = len(curdis[:])
        voronoi_kdtree = cKDTree(curdis)
        print size_data, size_model
        cov = np.cov(np.asarray(curdata/stdall).T)

        eucl_tot = 0
        maha_tot = 0
        nval = 0

        for i in range(size_data):
            #if curdata[i][0] < 3:
            #    continue
            vor_pt = np.asarray(curdata[i])/stdall
            test_point_dist, test_point_regions = voronoi_kdtree.query(vor_pt, k=1)
            model_pt = np.asarray(curdis[test_point_regions]) #find_nearest(curdis, vor_pt)#
            nval+=1            
            maha = scipy.spatial.distance.mahalanobis(vor_pt,model_pt,np.linalg.inv(cov))
            maha_tot += maha**5   #1/(maha**5+1) 
            dist.append(maha)
        poisson = scipy.stats.poisson.logpmf(size_data,size_model) #- scipy.stats.poisson.logpmf(size_data,size_data)
       # print poisson
        like_maha = -lbd * size_model/float(size_data)*maha_tot #size_model
        allz_maha += like_maha 
        allz_poiss+= poisson

    Lfinal.append(allz_maha+allz_poiss)
    LLfinal = allz_maha+allz_poiss
    Poissfinal.append(allz_poiss)
    Euclidfinal.append(allz_maha)
    print " *********************** "
   # print "Number of values combined: %ld"%nval
    print "Distance Maha %f"%(allz_maha)
    print "Value of the Poisson distribution: %f"%allz_poiss
    print "Likelihood(maha) : %f "%(LLfinal)#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))
    #print "Likelihood(maha) : %f "%(Lfinal[j])#-len(dis2[:]) + np.log(len(dis2[:]))*len(dis1[:]) - LogFactorial(int(len(dis1[:])))

    print " *********************** "

    print 
    print
#print nval,totp, np.log(scipy.stats.poisson.pmf(len(dis1[:]),len(dis2[:])))
#      *********************** 
# Distance Maha -13.012969
# Value of the Poisson distribution: -15.362372
# Likelihood(maha) : -28.375342 
#  *********************** 


plt.style.use('seaborn-deep')

#plt.hist([x, y], bins, label=['x', 'y'])
#plt.legend(loc='upper right')
plt.hist([Lfinal, saveL], bins= 8, label=['R = 15', 'R = 22'])
plt.legend(loc='upper left',fontsize =15)
plt
#plt.hist(saveL, bins = 10)
print np.max(Lfinal),np.max(saveL)


#print sum(i > -80 for i in Lfinal)
#print sum(i > -80 for i in saveL)
print np.median(Lfinal),np.mean(Lfinal), np.std(Lfinal)
print np.median(saveL),np.mean(saveL), np.std(saveL)
plt.ylabel('Count', fontsize = 15)
plt.xlabel('Log(LH)', fontsize = 15)
plt.tick_params(axis = 'both', which = 'both', labelsize = 15)


#plt.scatter(range(len(Lfinal)),Poissfina


plt.scatter(range(len(dist)), dist)
#plt.yscale('log')
plt.ylim(-0.01,0.1)
plt.figure()
plt.hist(dist, bins =1000)
plt.xlim(0,01)
print (len(dist) - np.count_nonzero(dist))/float(len(dist))
#dist = np.array(dist)
#print dist[np.array(dist)>0]
print np.min(dist[dist>1e-10])


saveL = Lfinal
saveP = Poissfinal
savem = Euclidfinal


print np.log(scipy.stats.poisson.pmf(5,6)/scipy.stats.poisson.pmf(5,5))
print np.log(scipy.stats.poisson.pmf(5,6))
print np.log(scipy.stats.poisson.pmf(50,51)/scipy.stats.poisson.pmf(50,50))
print np.log(scipy.stats.poisson.pmf(50,51))


noise = fits.open('21CMMC/21CMMC_Voronoi/Programs/NoiseData/Noise_map_ska/noise_arr_%1.6f.fits'%z)[0].data
print noise



